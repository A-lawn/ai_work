# Service Configuration
SERVICE_NAME=llm-service
SERVICE_PORT=9004
SERVICE_HOST=0.0.0.0

# Nacos Configuration
NACOS_SERVER=localhost:8848
NACOS_NAMESPACE=rag-system
NACOS_GROUP=DEFAULT_GROUP

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=1000
OPENAI_TIMEOUT=60

# Azure OpenAI Configuration (Optional)
AZURE_OPENAI_API_KEY=your-azure-api-key-here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT=gpt-4
AZURE_OPENAI_API_VERSION=2023-12-01-preview

# Local Model Configuration (Optional)
LOCAL_MODEL_ENDPOINT=http://localhost:8000
LOCAL_MODEL_NAME=local-model

# LLM Provider Selection (openai | azure | local)
LLM_PROVIDER=openai

# Token Configuration
MAX_CONTEXT_TOKENS=4000
MAX_RESPONSE_TOKENS=1000

# Retry Configuration
MAX_RETRIES=3
RETRY_DELAY=1.0

# Zipkin Configuration
ZIPKIN_ENDPOINT=http://localhost:9411/api/v2/spans
ENABLE_TRACING=true

# Prometheus Configuration
ENABLE_METRICS=true
